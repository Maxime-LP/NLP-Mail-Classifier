{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "inputText =\\\n",
    "\"\"\"\n",
    "Bonjour\n",
    "\n",
    "Lors de notre point mensuel sur la CSS je m’aperçois que sur le mois de mars nous avons énormément de tardifs qui ont été réglés sur mars en comparaison au mois de février. Je ne m’explique pas ce phénomène surtout que de ce que j’avais compris les non règlements du fait du pb avec OXANTIS ne concernaient que 5M€ au total et pour des soins 2019 et que cela devait finalement être récupéré sur le mois d’avril. De mon côté je ne pense pas avoir vu d’alerte sur des factures datant de 2015 et après qui n’auraient pas été réglées. D’où « sortent » ces règlements ?\n",
    "\n",
    "Je n’ai surement pas tout suivi dans le détail mais je trouve que les tardifs sont très élevés et il me sera demandé une explication par le Fonds CSS. Il est nécessaire je pense d’approfondir ce sujet car cela pourrait avoir des répercussions importantes sur notre définitif au niveau de la mutuelle si ce phénomène est généralisé\n",
    "\n",
    "   merci côté stats de me donner des précisions sur ce constat pour la CSS et si ceci est également constaté sur l’ensemble du portefeuille HM.\n",
    "\tEt en // merci à Nadège de regarder si tout est OK coté intégration dans DECIBEL\n",
    "\n",
    "D’avance je vous remercie pour toutes les précisions que vous pourrez m’apporter et je vous souhaite une bonne journée\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour Lors de notre point mensuel sur la CSS je aperçois que sur le mois de mars nous avons énormément de tardifs qui ont été réglés sur mars en comparaison au mois de février Je ne explique pas ce phénomène surtout que de ce que avais compris les non règlements du fait du pb avec OXANTIS ne concernaient que 5M au total et pour des soins 2019 et que cela devait finalement être récupéré sur le mois avril De mon côté je ne pense pas avoir vu alerte sur des factures datant de 2015 et après qui auraient pas été réglées où sortent ces règlements Je ai surement pas tout suivi dans le détail mais je trouve que les tardifs sont très élevés et il me sera demandé une explication par le Fonds CSS Il est nécessaire je pense approfondir ce sujet car cela pourrait avoir des répercussions importantes sur notre définitif au niveau de la mutuelle si ce phénomène est généralisé merci côté stats de me donner des précisions sur ce constat pour la CSS et si ceci est également constaté sur ensemble du portefeuille HM Et en merci Nadège de regarder si tout est OK coté intégration dans DECIBEL avance je vous remercie pour toutes les précisions que vous pourrez apporter et je vous souhaite une bonne journée \n",
      " Mean execution time : 0.009095828533172607\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.feature_extraction.text import HashingVectorizer,CountVectorizer,TfidfVectorizer\n",
    "tokenizer = TfidfVectorizer(strip_accents='ascii',lowercase=True).build_tokenizer()\n",
    "for k in range(10000):\n",
    "    words = tokenizer(inputText)\n",
    "    output = ' '.join(words)\n",
    "print(output,'\\n Mean execution time :',(time()-t0)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour Lors de notre point mensuel sur la CSS je aperçois que sur le mois de mars nous avons énormément de tardifs qui ont été réglés sur mars en comparaison au mois de février Je ne explique pas ce phénomène surtout que de ce que avais compris les non règlements du fait du pb avec OXANTIS ne concernaient que 5M au total et pour des soins 2019 et que cela devait finalement être récupéré sur le mois avril De mon côté je ne pense pas avoir vu alerte sur des factures datant de 2015 et après qui auraient pas été réglées où sortent ces règlements Je ai surement pas tout suivi dans le détail mais je trouve que les tardifs sont très élevés et il me sera demandé une explication par le Fonds CSS Il est nécessaire je pense approfondir ce sujet car cela pourrait avoir des répercussions importantes sur notre définitif au niveau de la mutuelle si ce phénomène est généralisé merci côté stats de me donner des précisions sur ce constat pour la CSS et si ceci est également constaté sur ensemble du portefeuille HM Et en merci Nadège de regarder si tout est OK coté intégration dans DECIBEL avance je vous remercie pour toutes les précisions que vous pourrez apporter et je vous souhaite une bonne journée \n",
      " Mean execution time : 0.008886620998382569\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "import re\n",
    "pattern = re.compile(r\"\\b\\w{2,}\\b\")\n",
    "for k in range(10000):\n",
    "    words = pattern.findall(inputText)\n",
    "    output = ' '.join(words)\n",
    "print(output,'\\n Mean execution time :',(time()-t0)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.fr import French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bonjour\n",
      "\n",
      "\n",
      "\n",
      "Lors\n",
      "de\n",
      "notre\n",
      "point\n",
      "mensuel\n",
      "sur\n",
      "la\n",
      "CSS\n",
      "je\n",
      "m’\n",
      "aperçois\n",
      "que\n",
      "sur\n",
      "le\n",
      "mois\n",
      "de\n",
      "mars\n",
      "nous\n",
      "avons\n",
      "énormément\n",
      "de\n",
      "tardifs\n",
      "qui\n",
      "ont\n",
      "été\n",
      "réglés\n",
      "sur\n",
      "mars\n",
      "en\n",
      "comparaison\n",
      "au\n",
      "mois\n",
      "de\n",
      "février\n",
      ".\n",
      "Je\n",
      "ne\n",
      "m’\n",
      "explique\n",
      "pas\n",
      "ce\n",
      "phénomène\n",
      "surtout\n",
      "que\n",
      "de\n",
      "ce\n",
      "que\n",
      "j’\n",
      "avais\n",
      "compris\n",
      "les\n",
      "non\n",
      "règlements\n",
      "du\n",
      "fait\n",
      "du\n",
      "pb\n",
      "avec\n",
      "OXANTIS\n",
      "ne\n",
      "concernaient\n",
      "que\n",
      "5M€\n",
      "au\n",
      "total\n",
      "et\n",
      "pour\n",
      "des\n",
      "soins\n",
      "2019\n",
      "et\n",
      "que\n",
      "cela\n",
      "devait\n",
      "finalement\n",
      "être\n",
      "récupéré\n",
      "sur\n",
      "le\n",
      "mois\n",
      "d’\n",
      "avril\n",
      ".\n",
      "De\n",
      "mon\n",
      "côté\n",
      "je\n",
      "ne\n",
      "pense\n",
      "pas\n",
      "avoir\n",
      "vu\n",
      "d’\n",
      "alerte\n",
      "sur\n",
      "des\n",
      "factures\n",
      "datant\n",
      "de\n",
      "2015\n",
      "et\n",
      "après\n",
      "qui\n",
      "n’\n",
      "auraient\n",
      "pas\n",
      "été\n",
      "réglées\n",
      ".\n",
      "D’\n",
      "où\n",
      "«\n",
      "sortent\n",
      "»\n",
      "ces\n",
      "règlements\n",
      "?\n",
      "\n",
      "\n",
      "\n",
      "Je\n",
      "n’\n",
      "ai\n",
      "surement\n",
      "pas\n",
      "tout\n",
      "suivi\n",
      "dans\n",
      "le\n",
      "détail\n",
      "mais\n",
      "je\n",
      "trouve\n",
      "que\n",
      "les\n",
      "tardifs\n",
      "sont\n",
      "très\n",
      "élevés\n",
      "et\n",
      "il\n",
      "me\n",
      "sera\n",
      "demandé\n",
      "une\n",
      "explication\n",
      "par\n",
      "le\n",
      "Fonds\n",
      "CSS\n",
      ".\n",
      "Il\n",
      "est\n",
      "nécessaire\n",
      "je\n",
      "pense\n",
      "d’\n",
      "approfondir\n",
      "ce\n",
      "sujet\n",
      "car\n",
      "cela\n",
      "pourrait\n",
      "avoir\n",
      "des\n",
      "répercussions\n",
      "importantes\n",
      "sur\n",
      "notre\n",
      "définitif\n",
      "au\n",
      "niveau\n",
      "de\n",
      "la\n",
      "mutuelle\n",
      "si\n",
      "ce\n",
      "phénomène\n",
      "est\n",
      "généralisé\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "merci\n",
      "côté\n",
      "stats\n",
      "de\n",
      "me\n",
      "donner\n",
      "des\n",
      "précisions\n",
      "sur\n",
      "ce\n",
      "constat\n",
      "pour\n",
      "la\n",
      "CSS\n",
      "et\n",
      "si\n",
      "ceci\n",
      "est\n",
      "également\n",
      "constaté\n",
      "sur\n",
      "l’\n",
      "ensemble\n",
      "du\n",
      "portefeuille\n",
      "HM\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\t\n",
      "Et\n",
      "en\n",
      "//\n",
      "merci\n",
      "à\n",
      "Nadège\n",
      "de\n",
      "regarder\n",
      "si\n",
      "tout\n",
      "est\n",
      "OK\n",
      "coté\n",
      "intégration\n",
      "dans\n",
      "DECIBEL\n",
      "\n",
      "\n",
      "\n",
      "D’\n",
      "avance\n",
      "je\n",
      "vous\n",
      "remercie\n",
      "pour\n",
      "toutes\n",
      "les\n",
      "précisions\n",
      "que\n",
      "vous\n",
      "pourrez\n",
      "m’\n",
      "apporter\n",
      "et\n",
      "je\n",
      "vous\n",
      "souhaite\n",
      "une\n",
      "bonne\n",
      "journée\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = French().tokenizer\n",
    "tokens = tokenizer(inputText)\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>new_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello there</td>\n",
       "      <td>(Hello, there)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi there</td>\n",
       "      <td>(Hi, there)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello world</td>\n",
       "      <td>(Hello, world)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text         new_col\n",
       "0  Hello there  (Hello, there)\n",
       "1     Hi there     (Hi, there)\n",
       "2  Hello world  (Hello, world)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "tokenizer = French().tokenizer\n",
    "\n",
    "df = pd.DataFrame.from_dict({'text':['Hello there','Hi there','Hello world']})\n",
    "df['new_col'] = df['text'].apply(lambda x: tokenizer(x))\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "444ad55116b32659c86cf21366f06a0b6b15c21184a40d72a1d77ccdcbfe1879"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
