{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from unidecode import unidecode\n",
    "from spacy.lang.fr import French\n",
    "from spacy.lang.fr import stop_words as spacy_stopwords\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3407, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>header</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yohan quand tu as le temps dis moi si c est cl...</td>\n",
       "      <td>1ère relecture gt conso</td>\n",
       "      <td>Fri, 29 May 2020 16:53:04 +0200</td>\n",
       "      <td>plonquet nadège</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1ère relecture gt conso yohan quand tu as le t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pour accepter la demande cliquez simplement su...</td>\n",
       "      <td>accepter</td>\n",
       "      <td>Mon, 16 Sep 2019 17:09:37 +0200</td>\n",
       "      <td>plonquet nadège</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>accepter pour accepter la demande cliquez simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bonjour pouvez vous donner les accès à decibel...</td>\n",
       "      <td>acces decibel</td>\n",
       "      <td>Tue, 23 Mar 2021 13:44:07 +0100</td>\n",
       "      <td>guillaume veronique</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>acces decibel bonjour pouvez vous donner les a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comme ça ne fonctionne toujours pas sur mon po...</td>\n",
       "      <td>accès rec4 decibel sdw rec4 hm dm ad restituti...</td>\n",
       "      <td>Mon, 24 Feb 2020 14:28:57 +0100</td>\n",
       "      <td>gueniot bernard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>accès rec4 decibel sdw rec4 hm dm ad restituti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tu sais ce que c est que ces actes qui n ont p...</td>\n",
       "      <td>actes indemnités hospitalières</td>\n",
       "      <td>Mon, 25 Mar 2019 11:25:36 +0100</td>\n",
       "      <td>levisse xavier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>actes indemnités hospitalières tu sais ce que ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  yohan quand tu as le temps dis moi si c est cl...   \n",
       "1  pour accepter la demande cliquez simplement su...   \n",
       "2  bonjour pouvez vous donner les accès à decibel...   \n",
       "3  comme ça ne fonctionne toujours pas sur mon po...   \n",
       "4  tu sais ce que c est que ces actes qui n ont p...   \n",
       "\n",
       "                                              header  \\\n",
       "0                            1ère relecture gt conso   \n",
       "1                                           accepter   \n",
       "2                                      acces decibel   \n",
       "3  accès rec4 decibel sdw rec4 hm dm ad restituti...   \n",
       "4                     actes indemnités hospitalières   \n",
       "\n",
       "                              date                 from  to  label  \\\n",
       "0  Fri, 29 May 2020 16:53:04 +0200      plonquet nadège NaN      0   \n",
       "1  Mon, 16 Sep 2019 17:09:37 +0200      plonquet nadège NaN      0   \n",
       "2  Tue, 23 Mar 2021 13:44:07 +0100  guillaume veronique NaN      1   \n",
       "3  Mon, 24 Feb 2020 14:28:57 +0100      gueniot bernard NaN      1   \n",
       "4  Mon, 25 Mar 2019 11:25:36 +0100       levisse xavier NaN      1   \n",
       "\n",
       "                                                text  \n",
       "0  1ère relecture gt conso yohan quand tu as le t...  \n",
       "1  accepter pour accepter la demande cliquez simp...  \n",
       "2  acces decibel bonjour pouvez vous donner les a...  \n",
       "3  accès rec4 decibel sdw rec4 hm dm ad restituti...  \n",
       "4  actes indemnités hospitalières tu sais ce que ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mails = pd.read_csv('mails.csv', index_col = 0, encoding='UTF-16', header=0, skipinitialspace=True, skip_blank_lines = True)\n",
    "mails['text'] = mails[['header', 'body']].astype(str).agg(' '.join, axis=1)\n",
    "print(mails.shape)\n",
    "mails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = French() ##charger le modèle ici\n",
    "stop_words = spacy_stopwords.STOP_WORDS\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonjour = nlp(\"bonjour\")\n",
    "bonjour.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    # sentence = nlp(sentence)\n",
    "    # lemmatizing\n",
    "    sentence = sentence.split(' ') #[unidecode(word.lemma_.lower()).strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in sentence ]\n",
    "    # removing stop words\n",
    "    sentence = [ word for word in sentence if word not in stop_words and word not in punctuations]        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = mails.text.progress_apply(tokenize)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for s in sentences:\n",
    "    vocab.update(set(s))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=[]\n",
    "for token in tqdm(vocab):\n",
    "    vectors.append(nlp(token).vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "\n",
    "x_axis = embeddings_2d[:, 0]\n",
    "y_axis = embeddings_2d[:, 1]\n",
    "\n",
    "#plt.scatter(x_axis, y_axis, s=5, alpha=0.5) # alpha for transparency\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage semi-supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyEmbeddings(TransformerMixin): # it inherits the sklearn's base class for transformers\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [sentence for sentence in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "bow_vector = CountVectorizer(tokenizer=tokenize, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unlabeled = mails[mails['label'] == 2]\n",
    "X, y = mails[mails['label'] != 2].text, mails[mails['label'] != 2].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9283489096573209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       830\n",
      "           1       0.71      0.81      0.76       133\n",
      "\n",
      "    accuracy                           0.93       963\n",
      "   macro avg       0.84      0.88      0.86       963\n",
      "weighted avg       0.93      0.93      0.93       963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipe = Pipeline([(\"embedder\", SpacyEmbeddings()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7518172377985463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       829\n",
      "           1       0.04      0.04      0.04       134\n",
      "\n",
      "    accuracy                           0.75       963\n",
      "   macro avg       0.45      0.45      0.45       963\n",
      "weighted avg       0.74      0.75      0.74       963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "pipe = Pipeline([(\"embedder\", SpacyEmbeddings()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', KMeans(n_clusters=2))])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210799584631361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95       833\n",
      "           1       0.71      0.70      0.71       130\n",
      "\n",
      "    accuracy                           0.92       963\n",
      "   macro avg       0.83      0.83      0.83       963\n",
      "weighted avg       0.92      0.92      0.92       963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipe = Pipeline([(\"embedder\", SpacyEmbeddings()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', SVC(C=0.1325, kernel = 'linear'))])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9086188992731049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       831\n",
      "           1       0.66      0.68      0.67       132\n",
      "\n",
      "    accuracy                           0.91       963\n",
      "   macro avg       0.81      0.81      0.81       963\n",
      "weighted avg       0.91      0.91      0.91       963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe = Pipeline([(\"embedder\", SpacyEmbeddings()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', LogisticRegression(C=1, solver = 'newton-cg'))])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction des labels manquants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3210, 7) (197, 7)\n",
      "(3407, 7) (0, 7) 0.9\n"
     ]
    }
   ],
   "source": [
    "estimator = Pipeline([(\"embedder\", SpacyEmbeddings()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', DecisionTreeClassifier())])  #SVC ou arbre\n",
    "\n",
    "X_unlabeled_copy = X_unlabeled.copy()\n",
    "newX = X\n",
    "newY = y\n",
    "threshold = 0.9\n",
    "print(newX.shape, X_unlabeled_copy.shape)\n",
    "while not X_unlabeled_copy.empty:\n",
    "\n",
    "    estimator.fit(newX.text, newY)\n",
    "    y_pred = estimator.predict(X_unlabeled_copy.text)\n",
    "    y_prob = estimator.predict_proba(X_unlabeled_copy.text)\n",
    "    X_unlabeled_copy['_pred_'] = y_pred\n",
    "    X_unlabeled_copy['_prob_'] = [y_prob[k,y_pred[k]] for k in range(len(y_pred))]\n",
    "    confident_pred = X_unlabeled_copy[X_unlabeled_copy['_prob_'] >= threshold]\n",
    "    X_unlabeled_copy = X_unlabeled_copy[X_unlabeled_copy['_prob_'] < threshold].drop(['_pred_', '_prob_'], axis = 1)\n",
    "    newX = pd.concat([newX, confident_pred.drop(['_pred_', '_prob_'], axis = 1)], ignore_index = True)\n",
    "    newY = pd.concat([newY, confident_pred['_pred_']], ignore_index = True)\n",
    "    print(newX.shape, X_unlabeled_copy.shape, threshold)\n",
    "\n",
    "    if len(confident_pred)==0:\n",
    "        break\n",
    "\n",
    "newX = newX.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catégorisation des mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9286412512218963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       874\n",
      "           1       0.76      0.74      0.75       149\n",
      "\n",
      "    accuracy                           0.93      1023\n",
      "   macro avg       0.86      0.85      0.86      1023\n",
      "weighted avg       0.93      0.93      0.93      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipe = Pipeline([(\"embedder\", SpacyEmbeddings()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size=0.3)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7341153470185728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85       858\n",
      "           1       0.03      0.02      0.03       165\n",
      "\n",
      "    accuracy                           0.73      1023\n",
      "   macro avg       0.43      0.45      0.44      1023\n",
      "weighted avg       0.70      0.73      0.71      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size=0.3)\n",
    "pipe = Pipeline([(\"embedder\", SpacyEmbeddings()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', KMeans(n_clusters=2))])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8983382209188661\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       859\n",
      "           1       0.67      0.73      0.70       164\n",
      "\n",
      "    accuracy                           0.90      1023\n",
      "   macro avg       0.81      0.83      0.82      1023\n",
      "weighted avg       0.90      0.90      0.90      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size=0.3)\n",
    "pipe = Pipeline([(\"embedder\", SpacyEmbeddings()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', SVC(C=0.1325, kernel = 'linear'))])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9198435972629521\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       869\n",
      "           1       0.78      0.65      0.71       154\n",
      "\n",
      "    accuracy                           0.92      1023\n",
      "   macro avg       0.86      0.81      0.83      1023\n",
      "weighted avg       0.92      0.92      0.92      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size=0.3)\n",
    "pipe = Pipeline([(\"embedder\", SpacyEmbeddings()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', LogisticRegression(C=1, solver = 'newton-cg'))])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1325, 'class_weight': None, 'kernel': 'linear'}\n",
      "0.9192010235179116\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size=0.3)\n",
    "\n",
    "gridCV = GridSearchCV(SVC(),\n",
    "    n_jobs=-1,\n",
    "    param_grid = {\n",
    "        'C' : [0.1325],\n",
    "        'kernel' : ['linear'],\n",
    "        #'degree' : [1, 2, 3, 4],\n",
    "        #'gamma' : ['scale', 'auto'] + list(np.linspace(0,2,3)),\n",
    "        #'coef0' : np.linspace(-2,2,5),\n",
    "        'class_weight' : ['balanced', None],\n",
    "        #'decision_function_shape' : ['ovo', 'ovr']\n",
    "    }\n",
    ")\n",
    "\n",
    "gridCV.fit(X_train, y_train)\n",
    "print(gridCV.best_params_)\n",
    "print(gridCV.best_score_)\n",
    "#{'C': 0.1325, 'class_weight': None, 'kernel': 'linear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9105691056910569"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc = SVC(C=0.1325, kernel = 'linear')\n",
    "model_svc.fit(X_train, y_train)\n",
    "y_pred = model_svc.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'svc_c01325_lin.sav'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model_svc, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size=0.3)\n",
    "\n",
    "gridCV2 = GridSearchCV(LogisticRegression(),\n",
    "    n_jobs=-1,\n",
    "    param_grid = {\n",
    "'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "'dual' : [False, True],\n",
    "'C' : np.linspace(0.0001,3, 10),\n",
    "'fit_intercept' : [True, False],\n",
    "'class_weight' : [None, 'balanced'],\n",
    "'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "'warm_start' : [True],\n",
    "'l1_ratio' : [0, 0.5, 1]\n",
    "    }\n",
    ")\n",
    "\n",
    "gridCV2.fit(X_train, y_train)\n",
    "print(gridCV2.best_params_)\n",
    "print(gridCV2.best_score_)\n",
    "#{'C': 0.3334222222222222, 'class_weight': None, 'dual': False, 'fit_intercept': False, 'l1_ratio': 0, 'penalty': 'l1', 'solver': 'liblinear', 'warm_start': True}\n",
    "#0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\le_paumier-m\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9196337741607324"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size=0.3)\n",
    "LR = LogisticRegression(C = 0.3334222222222222, class_weight = None, dual = True, fit_intercept = False, \n",
    "                        solver = 'liblinear', warm_start = True)\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lr_c033_liblinear_dual.sav'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(LR, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\le_paumier-m\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9116080937167199"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "LR = LogisticRegression(C = 0.3334222222222222, class_weight = None, dual = True, fit_intercept = False, \n",
    "                        solver = 'liblinear', warm_start = True)\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur ces méthodes de machine learning classique, l'extension du dataset à partir des données non-labellisées ne produit pas une grande amélioration des performances, ce sera plus flagrant lorsqu'on passera à des méthodes de Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction des probabilités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99, 0.01],\n",
       "       [0.78, 0.22],\n",
       "       [0.14, 0.86],\n",
       "       ...,\n",
       "       [0.94, 0.06],\n",
       "       [0.98, 0.02],\n",
       "       [0.66, 0.34]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(LR.predict_proba(newX),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple de pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lemmatization import lemmatizeText, initSpacy\n",
    "lemmatizer = initSpacy('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mail = \"\"\"\n",
    "    DRS - \n",
    "    Bonjour\n",
    "    j'ai un souci avec le décisionnel DECIBEL, il y a un problème de référenciel.\n",
    "\n",
    "    Cordialement,\n",
    "    Martin\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(mail, model_file, schema, lemmatizer):\n",
    "    features = defaultdict(int)\n",
    "    features.update(lemmatizeText(mail, lemmatizer))\n",
    "    X = np.array([features[key] for key in schema]).reshape(1, -1)    \n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "    try:\n",
    "        probas = model.predict_proba(X)[0]\n",
    "    except AttributeError:\n",
    "        probas = [None, None]\n",
    "    \n",
    "    y = model.predict(X)[0]\n",
    "    return {'predicted_class' : y, 'confidence' : probas[y], 'model' : model_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = 'lr_c1_newtoncg.sav'\n",
    "schema = X.columns.to_list()\n",
    "predicted_class = pipeline(input_mail, model_file, schema, lemmatizer)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to-do :\n",
    "\n",
    "* vote majoritaire entre plusieurs modèles\n",
    "* refit les modèles en infériorité numérique / refit périodiquement les moins bons modèles (p.r à un score de prédiction calculé sur la période)\n",
    "* gerer les cas douteux grace à la proba\n",
    "* LSTM / biLSTM\n",
    "* Transformers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "444ad55116b32659c86cf21366f06a0b6b15c21184a40d72a1d77ccdcbfe1879"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
